Лабораторная работа №3
======================


.. toctree::
    :maxdepth: 1
    :titlesonly:
    :caption: Full API

    lab_3_generate_by_ngrams.api.rst


Дано
----

1. Текст на английском языке - ``assets/Harry_Potter.txt``
   (загружен и сохранен в переменную ``text`` в ``start.py``).
2. Языковой профиль английского языка - ``assets/en_own.json``.

Генерация текста — одна из прикладных задач обработки естественного языка
(Natural Language Processing - NLP) за последние годы. На данный момент существует
большое количество больших языковой моделей, которые могут генерировать тексты,
близкие к написанным человеком.

Задача генерации текста включает в себя задачу обработки естественного языка
(Natural Language Processing, NLP) и реализует возможность языковой модели
на основе исходного текста предсказывать последующее слово и генерировать осмысленный
текст.

Существуют различные алгоритмы, позволяющие генерировать последовательности. По ходу
выполнения данной лабораторной работы Вы познакомитесь с тремя алгоритмами:

1. `Greedy Algorithm <https://en.wikipedia.org/wiki/Greedy_algorithm>`__
2. `Beam Search Algorithm <https://en.wikipedia.org/wiki/Beam_search>`__
3. `BackOff Algorithm <https://en.wikipedia.org/wiki/Exponential_backoff>`__

.. important:: В рамках данной лабораторной работы **нельзя использовать
               сторонние модули, а также стандартные модули collections и itertools**.

Терминология
------------

В рамках данной лабораторной вы будете работать с N-граммами.

**N-граммы** - это последовательность из n элементов, включенная в какую-либо другую
последовательность. В настоящей лабораторной мы будем работать с N-граммами, состоящими из
закодированных токенов текста.

Допустим, мы имеем следующий закодированный текст.
``(1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0)``

Каждое число в этой последовательности однозначно соответствут какому-либо токену.

Тогда биграммы (N-граммы размера 2) имеют следующий вид:

``((1, 2), (2, 3), (3, 0), (0, 4), (4, 1), (1, 0), (0, 2), (2, 5),
(5, 6), (6, 6), (6, 7), (7, 0), (0, 2), (2, 3), (3, 0), (0, 4), (4, 1), (1, 0),
(0, 2), (2, 5), (5, 6), (6, 6), (6, 7), (7, 0))``;

Иными словами, это все подпоследовательности длины 2, которые можно извлечь из этого текста.

Триграммы (N-граммы размера 3) имеют следующий вид:
``((1, 2, 3), (2, 3, 0), (3, 0, 4), (0, 4, 1), (4, 1, 0), (1, 0, 2),
(0, 2, 5), (2, 5, 6), (5, 6, 6), (6, 6, 7), (6, 7, 0), (7, 0, 2), (0, 2, 3),
(2, 3, 0), (3, 0, 4), (0, 4, 1), (4, 1, 0), (1, 0, 2), (0, 2, 5), (2, 5, 6),
(5, 6, 6), (6, 6, 7), (6, 7, 0))``

Работа с документацией
----------------------

В процессе чтения данной документации, Вы можете, нажав на метод,
изучить все аргументы, которые он принимает.

Что необходимо сделать
----------------------

Шаг 0. Начать работу над лабораторной (вместе с преподавателем на практике)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Изменить файлы ``main.py`` и ``start.py``.
2. Закоммитить изменения и создайте новый Pull Request.

.. important:: Код, выполняющий все требуемые действия должен быть написан в
               функции ``main`` в модуле ``start.py``. Для этого реализуйте функции в
               модуле ``main.py`` и импортируйте их в ``start.py``.

.. code:: py

   def main() -> None:
       pass

Вызов функции в файле ``start.py``:

.. code:: py

   if __name__ == '__main__':
       main()

**Обратите внимание**, что в файле ``target_score.txt`` необходимо выставить
желаемую оценку: 4, 6, 8 или 10. Чем выше желаемая оценка, тем большее
количество тестов запускается при проверке вашего Pull Request.

.. important:: В рамках данной лабораторной работы **нельзя использовать
               сторонние модули, а также стандартные модули collections и
               itertools.**

Шаг 1. Объявить сущность по обработке текста, кодированию и декодированию
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important:: Выполнение Шага 1 соответствует 4 баллам

Для работы с текстом в первую очередь необходимо научиться предобрабатывать сырые
текстовые данные. В этом нам поможет класс
:py:class:`lab_3_generate_by_ngrams.main.TextProcessor`, который вы реализуете в
ходе выполнения первого шага. В зону ответственности данного класса входят любые
манипуляции с текстом, включая его очистку, токенизацию, кодирование и декодирование.
Данный этап работы является ключевым, так как благодаря нему становится возможным
использование языковые модели, которые выявляют закономерности только в числовых данных.

Класс имеет следующие внутренние атрибуты:

* ``self._end_of_word_token`` - это специальный символ ``_``, обозначающий конец
  слова при посимвольной токенизации. Ожидаемый тип аргумента - строка;
* ``self._storage`` - словарь для хранения буквы-токена и ее идентификатора,
  который на данном этапе должен быть заполнен специальным символом и его
  идентификатором равным ``0``.

**Обратите внимание**, что оба эти атрибута являются защищенными, то есть обращение к ним за
пределами методов этого класса не предполагается.

Шаг 1.1. Токенизировать заданную последовательность
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor._tokenize`,
который позволяет разбить текст на токены текст.
Текст должен быть приведен к нижнему регистру и очищен от знаков препинания и цифр.
Токеном в данном случае является один буквенный символ.
В качестве разделителя между словами используется ``end_of_word_token``.

Метод разбивает текст на токены, вставляя после каждого слова специальный токен
конца слова ``end_of_word_token``, сохраненный в соответствующем атрибуте экземпляра
класса. Границей слова в данной работе выступает любой пробельный символ или их сочетание.

Если слово состоит полностью из цифр и знаков препинания, то вставлять токен конца
слова после него не нужно. Иными словами, в токенизированной последовательности не
должно быть двух токенов конца слова подряд.

.. note:: Разделитель добавляется после последнего слова, если текст заканчивается
          пробелом или знаком препинания. В противном случае, разделитель после последнего
          слова не добавляется.

Метод возвращает кортеж токенов.

Например, строка ``'She is happy. He is happy.'`` должна быть
токенизирована следующим образом:
``('s', 'h', 'e', '_', 'i', 's', '_', 'h', 'a', 'p', 'p', 'y', '_',
'h', 'e', '_', 'i', 's', '_', 'h', 'a', 'p', 'p', 'y', '_')``,

Строка ``'She is happy. He is happy'`` токенизируется так:
``('s', 'h', 'e', '_', 'i', 's', '_', 'h', 'a', 'p', 'p', 'y', '_',
'h', 'e', '_', 'i', 's', '_', 'h', 'a', 'p', 'p', 'y')``

.. note:: Если на вход подается аргумент неправильного типа, то есть не
          строка, или строка пустая, или если при токенизации не было найдено
          ни одной буквы, то возвращается значение ``None``.

Шаг 1.2. Добавить букву в хранилище
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

На этом шаге Вам нужно каждой букве из заданного текста
присвоить некоторый уникальный идентификатор.
Это необходимо для того, чтобы работать не со строками напрямую,
а с числами, которые их представляют.

Для этого реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor._put`.

.. note:: **Идентификатор** - значение, которое однозначно указывает
          на токен и равен длине ``_storage`` (атрибут объекта данного
          класса) на момент добавления буквы. Идентификатор специального
          символа ``_`` принимает значение ``0``.

**Правила корректного заполнения хранилища**:

* Идентификаторы уникальны и однозначно указывают на токен (например, при добавлении
  нового токена можно использовать длину хранилища `_storage`).
* Идентификатор специального символа `_` должен принимать значение `0`.

Основные условия:

* Для одной и той же буквы существует ровно один идентификатор;
* Одинаковых идентификаторов у двух разных букв быть не может;
* Если буква уже существовала в хранилище, идентификатор остается прежним.

Например, если на вход подается буква ``'s'``, то хранилище будет
выглядеть следующим образом - ``{'_': 0, 's': 1}``.

.. note:: Если на вход подается некорректное значение (аргумент
          неправильного типа, то есть не строка, или длина строки отлична от 1),
          возвращается ``None``.

**Обратите внимание**, что данный метод является защищенным: его использование за пределами
методов класса не предполагается.

Шаг 1.3. Получить идентификатор буквы
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Для успешного кодирования текста Вам необходимо научиться получать
идентификатор для каждой буквы из токенизированного текста.

Для этого реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.get_id`.

Например, в хранилище вида ``{'_': 0, 's': 1}`` для ``'s'``
метод вернет идентификатор ``1``.

.. note:: Если на вход подается некорректное значение аргумента
          (то есть тип аргумента не строка), или буква отсутствует
          в хранилище, возвращается None.

Шаг 1.4. Получить букву по идентификатору
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Теперь реализуем обратный процесс.

Для декодирования необходимо реализовать метод, который
получает букву по заданному идентификатору.

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.get_token`.

Например, в хранилище вида ``{'_': 0, 's': 1}`` для идентификатора
``1`` метод вернет ``'s'``.

.. note:: Если на вход подается неизвестный (отсутствует в хранилище)
          или некорректный (не соответствует типу данных
          ``int``) идентификатор, возвращается ``None``.

Шаг 1.5. Закодировать текст
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Наконец, мы можем реализовать метод, кодирующий текст.

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.encode`,
который обязательно должен вызывать методы
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor._tokenize`,
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor._put` и
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.get_id`.

Например, возьмем текст ``"She is happy. He is happy."`` и
заполним по нему хранилище:
``{'_': 0, 's': 1, 'h': 2, 'e': 3, 'i': 4, 'a': 5, 'p': 6, 'y': 7}``.
В результате кодирования текста выше должен получиться кортеж,
в котором каждый элемент соответствует идентификатору
буквы из хранилища ``_storage``:

.. code:: py

   encoded_corpus = (1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 3, 0, 4, 1, 0, 2, 5, 6,
                     6, 7, 0)

.. note:: Если на вход подаются некорректные значения (текст не является
          строкой или строка пустая), возвращается ``None``. Если какой-либо
          из методов, который вызывается в данном методе, возвращает ``None``,
          метод также должен вернуть ``None``.

Шаг 1.6. Декодирование текста в токенизированную последовательность
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Теперь, когда у нас есть выделенные токены и присвоенные им числовые
идентификаторы, мы можем декодировать любую последовательность.

Метод позволяет преобразовать закодированный текст в кортеж, состоящий
из буквенных и специальных символов.

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor._decode`,
который обязательно должен вызывать метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.get_token`.

Например, для закодированного корпуса
``(1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0)``
у вас должен получиться кортеж следующего вида:

.. code:: py

    decoded_corpus = ('s', 'h', 'e', '_', 'i', 's', '_', 'h', 'a', 'p', 'p', 'y', '_',
                      'h', 'e', '_', 'i', 's', '_', 'h', 'a', 'p', 'p', 'y', '_')

.. note:: Если на вход подаются некорректные значения (аргумент не является
          кортежем или кортеж пустой), если токенизированный текст или идентификатор
          буквы принимают значение ``None``, то метод возвращается ``None``.

**Обратите внимание**, что данный метод является защищенным: его использование за пределами
методов класса не предполагается.

Шаг 1.7. Декодировать текст в строковый формат
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor._postprocess_decoded_text`,
который позволяет перейти от токенизированного текста в формате
кортежа к тексту в строковом формате.

При этом на выходе строка должна соответствовать следующим требованиям:

1. Строка должна начинаться с заглавной буквы;
2. В конце строки ставится ``.``.

.. important:: Специальные токены конца слова должны быть конвертированы в пробелы.
               При этом пробел после последнего слова в тексте ставить не нужно.

Например, для декодированного на предыдущем шаге корпуса
``('s', 'h', 'e', '_', 'i', 's', '_', 'h', 'a', 'p', 'p', 'y', '_',
'h', 'e', '_', 'i', 's', '_', 'h', 'a', 'p', 'p', 'y', '_')``
должен получиться следующий текст: ``She is happy he is happy.``.

.. note:: Если на вход подаются некорректные значения (токенизированный
          текст не является кортежем или кортеж пустой), возвращается
          ``None``.

**Обратите внимание**, что данный метод является защищенным: его использование за пределами
методов класса не предполагается.

Шаг 1.8. Декодировать текст
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Наконец, реализуем полную логику перехода от закодированного текста к декодированному.

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.decode`,
который преобразует закодированный текст - кортеж с
идентификаторами - в текст в виде строки.

Метод обязательно должен вызывать методы
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor._decode` и
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor._postprocess_decoded_text`.

Например, для закодированного корпуса
``(1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0)``
у вас должен получиться следующий текст: ``She is happy he is happy.``.

.. note:: Если на вход подаются некорректные значения (закодированный
          текст не является кортежем или отсутствует), возвращается ``None``.
          Если какой-либо из использованных методов возвращает ``None``,
          необходимо так же вернуть ``None``.

Данный метод является публичным и инкапсулирует в себе внутреннюю логику
обработки текста.

Шаг 1.9. Получить специальный токен
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

В данном классе атрибут ``self._end_of_word_token`` является защищенным:
мы не хотим допустить его изменения в процессе использования внешним пользователем.
Однако узнать, какой именно токен используется для разделения слов, все-таки
иногда необходимо.

Для этого реализуем метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.get_end_of_word_token`,
который возвращает значение внутреннего атрибута ``self._end_of_word_token``.
Метод не принимает никаких аргументов.

Шаг 1.10. Продемонстрировать результаты на оценку 4 в ``start.py``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Продемонстрируйте результат кодирования и декодирования
в функции ``main()`` модуля ``start.py``, используя текст на
английском языке (переменная ``text``).
В качестве специального токена конца слова используйте строку ``'_'``.

Шаг 2. Создать структуру для хранения и обработки N-грамм
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important:: Выполнение Шагов 2 и 3 соответствует 6 баллам

Теперь у вас есть все, чтобы создать свою простейшую языковую модель на основе
N-грамм для решения задачи генерации текста.

Класс :py:class:`lab_3_generate_by_ngrams.main.NGramLanguageModel`
позволяет собрать N-граммы из заданного закодированного текста и
сгенерировать следующую букву последовательности.

Допустим, у нас есть закодированный текст, который выглядит
следующим образом: ``text = (1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0,
2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0)``.

Если нам необходимо заполнить ``NGramLanguageModel`` с ``N=2``,
то мы получим следующие биграммы: ``((1, 2), (2, 3), (3, 0), (0, 4),
(4, 1), (1, 0), (0, 2), (2, 5), (5, 6), (6, 6), (6, 7), (7, 0), (0, 2),
(2, 3), (3, 0), (0, 4), (4, 1), (1, 0), (0, 2), (2, 5), (5, 6), (6, 6),
(6, 7), (7, 0))``.
Если ``N=3``, то мы получим следующие триграммы:
``((1, 2, 3), (2, 3, 0), (3, 0, 4), (0, 4, 1), (4, 1, 0), (1, 0, 2),
(0, 2, 5), (2, 5, 6), (5, 6, 6), (6, 6, 7), (6, 7, 0), (7, 0, 2),
(0, 2, 3), (2, 3, 0), (3, 0, 4), (0, 4, 1), (4, 1, 0), (1, 0, 2),
(0, 2, 5), (2, 5, 6), (5, 6, 6), (6, 6, 7), (6, 7, 0))``.

Шаг 2.1. Объявить сущность языковой модели
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Теперь перейдем к реализации абстракции, которая хранит в себе языковое представление
в виде N-грамм и на их основе предсказывает следующий токен.

Создайте класс :py:class:`lab_3_generate_by_ngrams.main.NGramLanguageModel`.

Описание внутренних атрибутов:

* ``self._encoded_corpus`` - закодированный текст
* ``self._n_gram_size`` - размер N–грамм, который в данном случае должен принимать
  значения от 2 до 5.
* ``self._n_gram_frequencies`` - частотный словарь N–грамм, в котором ключами выступают
  N-граммы, а значениями вероятность появления последнего токена данной N-граммы в контексте,
  задаваемом N-граммой.

На момент инициализации атрибут ``self._n_gram_frequencies``` является пустым словарем,
его заполнение произойдет далее.

Шаг 2.2. Извлечь N-граммы из закодированного корпуса
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Как уже упоминалось, каждый экземпляр класса ``NGramLanguageModel`` содержит языковое
представление в виде N-грамм. Для этого нам необходимо извлечь из корпуса N-граммы
заданного размера.

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel._extract_n_grams`,
который извлекает из закодированного корпуса N-граммы, размер которых
указан в атрибуте ``self._n_gram_size``.

Например, для закодированного корпуса
``(1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0)``
при ``n_gram_size = 2`` метод должен вернуть следующий кортеж:
``((1, 2), (2, 3), (3, 0), (0, 4), (4, 1), (1, 0), (0, 2), (2, 5), (5, 6), (6, 6),
(6, 7), (7, 0), (0, 2), (2, 3), (3, 0), (0, 4), (4, 1), (1, 0), (0, 2), (2, 5),
(5, 6), (6, 6), (6, 7), (7, 0))``.

.. note:: Если на вход подаются некорректные значения (корпус не является
          кортежем или кортеж пустой), возвращается ``None``.

**Обратите внимание**, что данный метод является защищенным: его использование за
пределами методов класса не предполагается.

Шаг 2.3. Создать частотный словарь N-грамм
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel.build`,
который заполняет атрибут ``self._n_gram_frequencies``, где ключом
является N-грамма, а значением - частота N-граммы.

Для подсчета вероятности появления последнего токена данной N-граммы в контексте,
задаваемом N-граммой, используйте следующую формулу:

.. math::  \frac{P(w_{1},w_{2})}{P(w_{1})}

``P(w1}P(w2}`` - количество вхождений N-граммы, то есть её абсолютная частота.
``P(w1}`` - сколько раз в корпусе встречаются N-граммы, имеющие такое
же начало, то есть такие же первые N-1 символов.

Например, нам дан следующий набор триграмм:

.. code:: py

   n_grams = (
        (1, 2, 3), (2, 3, 0), (3, 0, 4), (0, 4, 1), (4, 1, 0),
        (1, 0, 2), (0, 2, 5), (2, 5, 6), (5, 6, 6), (6, 6, 7),
        (6, 7, 0), (7, 0, 2), (0, 2, 3), (2, 3, 0), (3, 0, 4),
        (0, 4, 1), (4, 1, 0), (1, 0, 2), (0, 2, 5), (2, 5, 6),
        (5, 6, 6), (6, 6, 7), (6, 7, 0))

Тогда, для триграммы ``(0, 2, 5)`` значение ``P(w1}P(w2}`` равно ``2``,
так как данная N-грамма только один раз встретилась в кортеже со всеми N-граммами.

Значение ``P(w1}`` равно ``3``, так как именно столько раз N-грамма
``(0, 2)``, имеющие такое же начало, встретилась среди всех N-грамм,
содержащих первые N-1 символов.

Следовательно, для N-граммы ``(0, 2, 5)`` частотный словарь будет заполнен
следующем образом:

.. code:: py

   frequencies = {
        (0, 2, 5): 0.6666666666666666}


.. note:: Если метод принимает на вход закодированный текст не в виде кортежа или
          если кортеж пустой, а также, если некорректно извлекаются N-граммы,
          то возвращается ``1``, если происходит корректное заполнение частотного
          словаря, метод возвращает ``0``

Метод обязательно должен вызывать
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel._extract_n_grams`.

Например, для закодированного корпуса
``(1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0)``
при ``n_gram_size = 3`` должен получиться частотный словарь вида:

.. code:: py

   frequencies = {
        (1, 2, 3): 1.0,
        (2, 3, 0): 1.0,
        (3, 0, 4): 1.0,
        (0, 4, 1): 1.0,
        (4, 1, 0): 1.0,
        (1, 0, 2): 1.0,
        (0, 2, 5): 0.6666666666666666,
        (2, 5, 6): 1.0,
        (5, 6, 6): 1.0,
        (6, 6, 7): 1.0,
        (6, 7, 0): 1.0,
        (7, 0, 2): 1.0,
        (0, 2, 3): 0.3333333333333333}

Продемонстрируйте результат работы данного метода в функции ``main()`` модуля
``start.py``.

Шаг 2.4. Получить размер N-грамм
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

В данном классе атрибут ``self._n_gram_size`` является защищенным:
мы не хотим допустить его изменения в процессе использования внешним пользователем.
Однако узнать, какой именно токен используется для разделения слов, все-таки
иногда необходимо.

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel.get_n_gram_size`,
который возвращает значение внутреннего атрибута ``self._n_gram_size``.

Шаг 2.5. Сгенерировать следующий токен
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Для генерации текста Вам необходимо научиться по заданному
контексту определять следующую букву в последовательности.

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel.generate_next_token`.

Данный метод принимает на вход последовательность закодированных токенов.
Метод отсекает контекст, по которому необходимо определить вероятность каждого из потенциальных
следующих токенов, то есть N-1 последних элементов последовательности. N в данном случае - размер
N-грамм, хранимых моделью.

Например, для N-граммы ``(1, 2, 3, 0)``, контекстом является ``(2, 3, 0)``.

Ваша задача, используя частотный словарь и извлеченный контекст, выделить все N-граммы,
начинающиеся с данного контекста.
Последние элементы выделенных N-грамм и будут токенами, которые могут продолжить данную
последовательность
Метод возвращает словарь, ключами которого являются буквы, а
значениями частоты N-грамм, из которых данная буква была выявлена по контексту.

Например, метод принимает на вход следующую последовательность: ``(7, 5, 6, 6)``.
Для данной последовательности при ``n_gram_size = 3`` контекстом является
``(6, 6)``.

Пусть мы имеем частотный словарь следующего вида:

.. code:: py

   frequencies = {
        (1, 2, 3): 1.0,
        (2, 3, 0): 1.0,
        (3, 0, 4): 1.0,
        (0, 4, 1): 1.0,
        (4, 1, 0): 1.0,
        (1, 0, 2): 1.0,
        (0, 2, 5): 0.6666666666666666,
        (2, 5, 6): 1.0,
        (5, 6, 6): 1.0,
        (6, 6, 7): 1.0,
        (6, 7, 0): 1.0,
        (7, 0, 2): 1.0,
        (0, 2, 3): 0.3333333333333333}

Тогда, мы имеем только одну N-грамму, а именно ``(6, 6, 7)`` с частотой
``1.0``, в которой присутствует контекст ``(6, 6)``.

Метод должен вернуть следующий словарь: ``{7: 1.0}``,
в котором ключ - буква, найденная по контексту ``(6, 6)``, а
значение - частота N-граммы ``(6, 6, 7)``.

В словаре таких пар ключ-значение может быть несколько,
поэтому необходимо произвести двойную сортировку пар: по значению и по ключу
в порядке убывания.

.. note:: Если введенная последовательность не является кортежем, кортеж пустой или
          неправильная длина последовательности (длина последовательности меньше, чем
          длина контекста), возвращается ``None``.

Шаг 3. Создать жадный генератор текста
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Шаг 3.1. Объявить сущность генератора
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Теперь мы полностью готовы к реализации генерации текста. Начнем с самого простого принципа
генерации.

Жадный алгоритм - это алгоритм генерации, в котором следующим токеном всегда выбирается
наиболее вероятный элемент, предполагая, что конечное решение также будет наиболее вероятный.

Для того, чтобы сгенерировать последовательности заданной длины,
создайте класс :py:class:`lab_3_generate_by_ngrams.main.GreedyTextGenerator`.

Описание внутренних атрибутов:

* ``self._model`` - экземпляр класса ``NGramLanguageModel``
* ``self._text_processor`` - экземпляр класса ``TextProcessor``

Шаг 3.2. Сгенерировать последовательность
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.GreedyTextGenerator.run`,
который генерирует последовательность указанной длины.

Прежде, чем создать жадный алгоритм генерации, необходимо сделать несколько
подготовительных преобразований:

* Закодировать заданную последовательность (``prompt``), используйте для этого экземпляр
  класса ``TextProcessor``, хранимый в соответствующем атрибуте;
* Получить из языковой модели контекст для генерации. Напоминаем, что длина контекста зависит от
  размера N-грамм.

Алгоритм создания жадного генератора:

1. Получить все буквы-кандидаты по контексту. Используйте уже реализованный метод
   класса ``NGramLanguageModel``;
2. Сгенерировать указанное количество букв, каждый раз выбирая
   кандидата с наибольшей частотой из словаря и добавляя его к последовательности.

Следует учесть, что:

1. В случае, если буквы-кандидаты не были найдены, то генерация прекращается,
   чтобы избежать процесса зацикливания;
2. С добавлением новой буквы к исходной последовательности,
   контекст изменяется, так как меняется последовательность.

Метод возвращает сгенерированный текст в виде строки.

.. note:: Если на вход подаются некорректные значения (длина последовательности
          не типа ``int``, заданная последовательность не является
          строкой или последовательность пустая), если вызываемые методы возвращают
          значение ``None``, метод возвращает значение ``None``.

В данном методе необходимо использовать методы
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.encode` и
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.decode`,
а также методы
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel.get_n_gram_size` и
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel.generate_next_token`.

Например, при следующих значениях:

.. code:: py

   seq_len = 6
   prompt = 'She is'

метод возвращает следующий результат ``'She is happy.'``.

Шаг 3.3. Продемонстрировать результаты на оценку 6 в ``start.py``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Продемонстрируйте результат работы жадного алгоритма генерации текста
в функции ``main()`` модуля ``start.py``. Попробуйте в качестве затравки использовать 'Vernon',
пусть размер N-грамм будет равен 7, а длина последовательности 51 .

Шаг 4. Создание алгоритма Beam Search
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important:: Выполнение Шагов 4 и 5 соответствует 8 баллам

На данном этапе мы предлагаем Вам реализовать генератор, используя алгоритм лучевого
поиска.
Принципиальное отличие данного алгоритма от жадного заключается в том, что для
жадного алгоритма оптимальная последовательность может быть достигнута, даже если
на каком-то шаге алгоритм выберет не самое вероятное продолжение, в то время, как
лучевой поиск строит дерево поиска. На каждом уровне дерева алгоритм генерирует
все возможные варианты продолжения последовательности, сортируя их в порядке
убывания вероятности. Для того, чтобы не происходило сильное ветвление дерева,
на каждом уровне выбирается заранее определенное количество наиболее вероятных
вариантов, которое называется **шириной луча**. Далее разворачиваются только эти
состояния. При ширине луча ``1`` лучевой поиск идентичен жадному поиску.

Вы спросите, почему мы не генерируем все возможные варианты продолжения
последовательности, чтобы затем выбирать наилучшую. Ответ: подобный алгоритм
является очень дорогим с точки зрения вычислительных ресурсов.

Именно поэтому алгоритм Beam Search с шириной луча - компромисс.

Рассмотрим работу алгоритма лучевого поиска на примере:

Допустим, нам дана следующая закодированная последовательность, которую необходимо
продолжить:

.. code:: py

   encoded_prompt = (1, 2, 3, 0, 4, 1)

следующий частотный словарь N-грамм при ``n_gram_size = 3``:

.. code:: py

   frequencies = {
        (1, 2, 3): 1.0,
        (2, 3, 0): 1.0,
        (3, 0, 4): 1.0,
        (0, 4, 1): 1.0,
        (4, 1, 0): 1.0,
        (1, 0, 2): 1.0,
        (0, 2, 5): 0.6666666666666666,
        (2, 5, 6): 1.0,
        (5, 6, 6): 1.0,
        (6, 6, 7): 1.0,
        (6, 7, 0): 1.0,
        (7, 0, 2): 1.0,
        (0, 2, 3): 0.3333333333333333}

и значение ширины луча: ``beam width = 3``.

1. Сначала языковая модель выявляет контекст из закодированной последовательности,
   который в данном случае имеет следующий вид: ``(4, 1)``.

2. В соответствии с частотным словарем, есть только одна буква-кандидат (удовлетворяет
   заданному значению ширины луча) для продолжения контекста: ``0`` с частотой
   встречаемости N–граммы ``0.08695652173913043``.

3. Таким образом, у нас получается только один вариант продолжения последовательности
   (дерево ветвится только на одно состояние, что удовлетворяет заданному значению ширины луча)
   с вероятностью, которая в нашем случае считается, как разность вероятности исходной
   последовательности и логарифмированной частоты N–граммы:
   ``{(1, 2, 3, 0, 4, 1, 0): 2.4423470353692043}``.

.. important:: Изначальная вероятность последовательности (в данном примере
               последовательностью является ``(1, 2, 3, 0, 4, 1)``) принимает значение
               ``0.0``.

Теперь нам снова необходимо получить контекст с помощью языковой модели из обновленной
закодированной последовательности, который в данном случае имеет следующий вид: ``(1, 0)``.

Далее шаги 2 и 3 повторяются до тех пор, пока не будет достигнута желаемая
длина последовательности.

.. important:: Обратите внимание, что на шагах 2 и 3 количество рассматриваемых для дальнейшей
               генерации букв-кандидатов и вариантов продолжения последовательности не должно
               превышать значение ширины луча.

Шаг 4.1. Объявить сущность генератора
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Теперь Ваша задача - написать алгоритм лучевого поиска.
Создайте класс :py:class:`lab_3_generate_by_ngrams.main.BeamSearcher`.

Описание внутренних атрибутов класса:

* ``self._beam_width`` - ширина луча, заполняется значением аргумента ``beam_width``
* ``self._model`` - экземпляр класса ``NGramLanguageModel``, заполняется значением
  аргумента ``language_model``.

Шаг 4.2. Получить буквы-кандидаты для генерации
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.BeamSearcher.get_next_token`,
который позволит Вам получать следующую букву для генерации и вероятность
появления этого токена в заданном контексте.

.. important:: Помните, что количество возвращаемых букв-кандидатов и не должно
               превышать значение ширины луча.

Например, если метод получает на вход следующую последовательность:

.. code:: py

   sequence = (1, 2, 3, 0, 4, 1)

контекстом для которой является кортеж ``(4, 1)``,

и частотный словарь хранит следующие значения при ``n_gram_size = 3``:

.. code:: py

   frequencies = {
        (1, 2, 3): 1.0,
        (2, 3, 0): 1.0,
        (3, 0, 4): 1.0,
        (0, 4, 1): 1.0,
        (4, 1, 0): 1.0,
        (1, 0, 2): 1.0,
        (0, 2, 5): 0.6666666666666666,
        (2, 5, 6): 1.0,
        (5, 6, 6): 1.0,
        (6, 6, 7): 1.0,
        (6, 7, 0): 1.0,
        (7, 0, 2): 1.0,
        (0, 2, 3): 0.3333333333333333}

то метод возвращает: ``[(0, 1.0)]``

.. note:: Если аргументы имеют некорректный тип данных, на вход подается пустая
          последовательность или метод ``generate_next_token`` возвращает
          ``None``, то данный метод возвращает ``None``. В случае, если словарь с буквами
          для генерации пустой, то метод возвращает пустой список.

Данный метод обязательно должен вызывать метод
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel.generate_next_token`.

Шаг 4.3. Получить варианты итоговых последовательностей
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.BeamSearcher.continue_sequence`.
Данный метод позволяет получить варианты продолжения последовательностей.

.. note:: Если аргументы имеют некорректный тип данных, на вход подаются пустые
          аргументы, длина списков букв-кандидатов превышает значение ширины луча
          или последовательность, которую мы хотим продолжить, отсутствует в
          последовательностях-кандидатах, то метод возвращается ``None``.

В данном методе необходимо заполнить существующий словарь итоговых
последовательностей-кандидатов новыми последовательностями, которые состоят
из текущей последовательности и нового токена.

Для каждой новой последовательности в словаре необходимо посчитать ее вероятность
следующим образом: разность вероятности текущей последовательности (то есть
последовательности, которую мы расширяем новой буквой) и логарифмированной частоты
буквы-кандидата.

Почему нам необходимо логарифмировать частоту встречаемости буквы?

В данном случае нам необходимо получить вероятность события, когда текущая
последовательность, которую мы хотим продолжить, встречается с буквой-кандидатом.
Данная вероятность вычисляется как произведение вероятностей текущей последовательности
и буквы. Но, так как вероятность текущей последовательности и вероятность
буквы изменяются в диапазоне от ``0`` до ``1``, то при перемножении вероятности N–грамм
становятся настолько маленькими, что все обращаются в ``0``. В таком случае возникает
проблема максимального правдоподобия.
Чтобы избежать побочного эффекта от данной ситуации, необходимо применить логарифмирование.
Логарифм преобразует результат из произведения в сумму, что значительно упрощает
дальнейший анализ.
Результаты не являются численно стабильными, они имеют тенденцию либо быстро сходиться к
нулю, либо к бесконечности, в зависимости от того, принимает ли вероятность значение меньше
или больше ``1``.
Более того, логарифм инверсирует порядок, то есть ``(log(0.2) > log(0.8))``,
поэтому при вычислении вероятности мы берем ``-log``.

Следует учесть, что:

* Словарь с итоговыми последовательностями-кандидатами изменяется внутри метода,
  являясь одновременно и входным, и выходным значением. Хорошим тоном является создание
  функции, которая не изменяет объекты за ее пределами, а так как словарь - изменяемый тип
  данных, манипуляции с ним отразятся на объекте за пределами функции. Подумайте, как можно
  избежать побочного эффекта в данном случае;
* В словарях с последовательностями-кандидатами необходимо удалять элемент
  с текущей последовательностью (то есть с последовательностью до расширения), так как
  мы уже расширили её и вероятность её расширенных версий будет выше.

Метод возвращает словарь из новых последовательностей-кандидатов и вероятностью
встречаемости.

Например, метод принимает на вход следующие значения аргументов:

.. code:: py

   sequence = (1, 2, 3, 0, 4, 1, 0, 2)
   next_tokens = [(5, 0.6666666666666666), (3, 0.3333333333333333)]
   sequence_candidates = {(1, 2, 3, 0, 4, 1, 0, 2): 0.0}

и выводит следующий словарь с последовательностями-кандидатами:

.. code:: py

   sequence_candidates = {
        (1, 2, 3, 0, 4, 1, 0, 2, 5): 0.40546510810816444,
        (1, 2, 3, 0, 4, 1, 0, 2, 3): 1.0986122886681098}

Шаг 4.4. Удаление последовательностей, которые не соответствуют значению ширины луча
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.BeamSearcher.prune_sequence_candidates`

Данный метод фильтрует недостаточно вероятные последовательности.

.. important:: Помните, что количество итоговых последовательностей
               не должно превышать значение ширины луча.

Метод принимает на вход последовательность кандидатов ``sequence_candidates`` в виде словаря,
ключом которого является последовательность, а значением вероятность встречаемости данной
последовательности.

.. note:: В случае, если подаются некорректные значения (последовательность не является
          словарем или последовательность пустая), возвращаемое значение `None`.

Если значения корректные, то необходимо:

1. Произвести двойную сортировку последовательностей-кандидатов следующим образом:
   по значению и по ключу в порядке убывания;
2. Удалить последовательности, которые не входят в топ-N наиболее вероятных, где
   N ширина луча.

Метод возвращает словарь, ключами которого являются последовательности, которые
входят в топ-N наиболее вероятных, а значениями их вероятность.

.. important:: В силу применения - логарифма, наиболее вероятными последовательностями
               являются те, у которых соответствующее значение ниже.

Количество топ-N последовательностей определяется параметром `beam_width`.

Например, на вход подается следующая последовательность кандидатов:

.. code:: py

    sequence_candidates = {
        (1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 5): 0.8109302162163289,
        (1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 3): 1.5040773967762742,
        (1, 2, 3, 0, 4, 1, 0, 2, 3, 0, 4, 1, 0, 2, 5): 1.5040773967762742,
        (1, 2, 3, 0, 4, 1, 0, 2, 3, 0, 4, 1, 0, 2, 3): 2.1972245773362196}

* после сортировки получаем следующий список последовательностей:

.. code:: py

    sorted_sequences = [
        (1, 2, 3, 0, 4, 1, 0, 2, 3, 0, 4, 1, 0, 2, 3),
        (1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 3),
        (1, 2, 3, 0, 4, 1, 0, 2, 3, 0, 4, 1, 0, 2, 5),
        (1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 5)]

* кандидаты на удаление при ``beam_width = 3``:

.. code:: py

    sequences_to_remove = [(1, 2, 3, 0, 4, 1, 0, 2, 3, 0, 4, 1, 0, 2, 3)]

В результате метод выведет следующий словарь:

.. code:: py

    result = {
        (1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 5): 0.8109302162163289,
        (1, 2, 3, 0, 4, 1, 0, 2, 5, 6, 6, 7, 0, 2, 3): 1.5040773967762742,
        (1, 2, 3, 0, 4, 1, 0, 2, 3, 0, 4, 1, 0, 2, 5): 1.5040773967762742}

Шаг 5. Создание генератора текста на основе алгоритма Beam Search
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Шаг 5.1. Объявить сущность генератора
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Теперь у вас есть все, чтобы создать генератор.
Создайте класс :py:class:`lab_3_generate_by_ngrams.main.BeamSearchTextGenerator`.

Описание внутренних атрибутов класса:

* ``self._text_processor`` - экземпляр класса ``TextProcessor``
* ``self._beam_width`` - ширина луча
* ``self.beam_searchers`` - экземпляр класса ``BeamSearcher``. В качестве аргументов
  экземпляр класса принимает значение атрибута ``self._beam_width`` и языковую модель.

.. important:: Экземпляры инициализируются в ``__init__``.

Шаг 5.2. Получить следующий токен
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Теперь ваша задача - создать генератор на основе алгоритма Beam Search.

Для того, чтобы получать следующую букву для генерации последовательности
необходимо вызвать метод для нулевого экземпляра класса ``BeamSearcher``,
который хранится в атрибуте ``self.beam_searchers`` данного класса.

Реализуйте метод

:py:meth:`lab_3_generate_by_ngrams.main.BeamSearchTextGenerator._get_next_token`.

В данном методе необходимо использовать метод
:py:meth:`lab_3_generate_by_ngrams.main.BeamSearcher.get_next_token`

.. note:: Если аргумент имеет некорректный тип данных, то есть не
          является кортежем, или кортеж пустой, то метод возвращается ``None``.
          Если используемые методы возвращают ``None``, то метод также возвращает
          ``None``.

Шаг 5.3. Генерация последовательности
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуйте метод, который принимает на вход количество букв для генерации
и начало последовательности в строковом виде. Данный метод позволяет получить
готовый сгенерированный текст.

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.BeamSearchTextGenerator.run`.

Создание генератора:

1. Создайте словарь с последовательностями-кандидатами, ключом которого изначально
   является закодированное начало последовательности, а значением является частота
   закодированной последовательности, принимающая значение ``0.0``;
2. Получите топ-N (где N ширина луча) букв-кандидатов для продолжения текущей последовательности;
3. Получите все вариантов последовательностей для дальнейшей генерации. Используйте
   для реализации данного пункта экземпляр класса ``BeamSearcher``;
4. Фильтрация последовательностей, то есть удаление всех тех последовательностей, которые не
   являются топ-N (где N ширина луча). Теперь это новые последовательности-кандидаты.
   Используйте для реализации данного пункта нулевой экземпляр класса ``BeamSearcher``;
5. Повторяйте шаги 3-5 до тех пор, пока не будет достигнута желаемая длина сгенерированной
   последовательности;
6. Выберите наилучшую из всех последовательностей. Помните, что максимальной вероятности
   соответствует минимальное значение);
7. Декодируйте наилучшую последовательность в текст.

В данном методе необходимо использовать методы
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.encode` и
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.decode`,
а также методы
:py:meth:`lab_3_generate_by_ngrams.main.BeamSearcher.get_next_token`,
:py:meth:`lab_3_generate_by_ngrams.main.BeamSearcher.continue_sequence` и
:py:meth:`lab_3_generate_by_ngrams.main.BeamSearcher.prune_sequence_candidates`.

.. note:: Если на вход подаются некорректные значения (количество букв для
          генерации не является целым числом или значение является неположительным,
          а также последовательность не является строкой или строка пустая),
          то метод возвращается ``None``.
          Если любой используемые методы возвращают ``None``, то метод также возвращает
          ``None``.

Например, метод принимает на вход длину генерируемой последовательности равную ``1`` и
начало последовательности ``'She i'``.

Мы имеем частотный словарь для триграмм и значение ширины луча:

.. code:: py

   frequencies = {
        (1, 2, 3): 1.0,
        (2, 3, 0): 1.0,
        (3, 0, 4): 1.0,
        (0, 4, 1): 1.0,
        (4, 1, 0): 1.0,
        (1, 0, 2): 1.0,
        (0, 2, 5): 0.6666666666666666,
        (2, 5, 6): 1.0,
        (5, 6, 6): 1.0,
        (6, 6, 7): 1.0,
        (6, 7, 0): 1.0,
        (7, 0, 2): 1.0,
        (0, 2, 3): 0.3333333333333333}
    beam_width = 3

Закодированная начальная последовательность имеет следующий вид: ``(1, 2, 3, 0, 4)``.
Следовательно, модель должна выявить следующий контекст: ``(0, 4)``.

Из частотного словаря получаем, что кандидат, чтобы продолжить последовательность у
нас один: ``[(1, 1.0)]``.

В таком случае, текущим кандидатом является последовательность ``(1, 2, 3, 0, 4, 1)``.
Так как кандидат один, то нет кандидатов для удаления (в случае, если кандидатов было
четыре, то после их сортировки необходимо было оставить количество, заданной шириной луча).

Метод возвращает следующий результат ``'She is.'``.

Шаг 5.4. Продемонстрировать результаты на оценку 8 в ``start.py``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Продемонстрируйте результат работы алгоритма лучевого поиска
в функции ``main()`` модуля ``start.py``. Попробуйте в качестве затравки использовать
``'Vernon'``, пусть размер N-грамм будет равен ``7``,
а длина последовательности ``56``.

Шаг 6. Заполнить хранилище новыми N-граммами
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important:: Выполнение Шагов 6-8 соответствует 10 баллам

Прежде чем приступить к созданию генератора текста на основе алгоритма
BackOff, мы предлагаем Вам расширить хранилище с токенами и их идентификаторами
новыми значениями, которые хранятся в виде словаря в файле с расширением ``.json``.
Реализация данного шага позволит Вам улучшить результат генерации, так как
словарь составлен на большом текстовом материале.

Вам необходимо расширить класс ``TextProcessor`` новым методом
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.fill_from_ngrams`

В данном методе, получая на вход словарь из файла с языковым профилем английского
языка, вам необходимо:

1. Отобрать только те N-граммы, длина которых равна ``1``;
2. Если N-грамма является буквой, то необходимо этой буквой пополнить хранилище
   ``_storage``.

В данном методе необходимо использовать метод
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor._put`

.. note:: Если на вход подается некорректное значение (аргумент
          неправильного типа, то есть не словарь, или словарь пустой),
          возвращается ``None``.

Например, если метод принимает на вход следующий словарь:

.. code:: py

   content = {
        "freq": {
                "h ap": 8629,
                " ape": 3042,
                "apex": 624}}

То в результате должно получиться хранилище, заполненное следующим образом:

.. code:: py

   storage = {'_': 0, 'h': 1, 'a': 2, 'p': 3, 'e': 4, 'x': 5}

Данный метод ничего не возвращает.

Шаг 7. Установить значение атрибута ``self._n_gram_frequencies``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Вам необходимо расширить класс ``NGramLanguageModel`` новым методом
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel.set_n_grams`.

Данный метод позволяет присвоить атрибуту ``self._n_gram_frequencies``
получаемое на вход значение.

.. note:: Если на вход подается некорректное значение (аргумент
          неправильного типа, то есть не словарь, или словарь пустой),
          возвращается ``None``.

Шаг 8. Создание генератора текста на основе алгоритма BackOff
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Теперь у Вас есть все, чтобы реализовать последний в рамках данной лабораторной
алгоритм генерации текста, а именно алгоритм BackOff.

Принципиальное отличие данного алгоритма заключается в том, что мы получаем
на вход не одну языковую модель, а сразу несколько и при генерации текста выбираем
наиболее вероятную букву-кандидата, анализируя все возможные N-граммы.

Шаг 8.1. Объявить сущность по расширению языковой модели на основе N-грамм
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Создайте класс :py:class:`lab_3_generate_by_ngrams.main.NGramLanguageModelReader`.

Описание внутренних атрибутов класса:

* ``self._json_path`` - путь к файлу с расширением ``.json``, который
  содержит языковой профиль английского языка (``en_own.json``)
* ``self._eow_token`` - специальный символ конца слова в формате строки
* ``self._content`` - содержимое файла с расширением ``.json``. Для чтения
  и сохранения файлов данного типа используйте стандартный модуль ``json``.
* ``self._text_processor`` - экземпляр класса ``TextProcessor``.

На данном шаге Вам также необходимо заполнить хранилище ``_storage``, используя
содержимое файла с расширением ``.json``, которое хранится в атрибуте
``self._content``.

Шаг 8.1.1 Получить экземпляр класса ``NGramLanguageModel``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModelReader.load`.

Данный метод позволяет получить языковую модель для определенного размера
N-грамм.

Во-первых, необходимо каждую N-грамму, полученную из файла ``.json``,
привести к определенному формату:

* Заменить пробельные символы на специальный символ конца слова;
* Выбрать только те токены в N-грамме, которые являются буквой или специальным
  символом;
* Привести к нижнему регистру.

Если заданный размер N-грамм соотносится с длиной N-грамм из файла, то необходимо
кодировать N-грамму из языкового профиля.

Создайте частотный словарь с N-граммами из файла и вероятностями
появления последнего токена данной N-граммы в контексте, задаваемом N-граммой.
Воспользуйтесь формулой условной вероятности:

.. math::  \frac{P(w_{1},w_{2})}{P(w_{1})}

``P(w1}P(w2}`` - количество вхождений N-граммы, то есть её абсолютная частота.
``P(w1}`` - сколько раз в корпусе встречаются N-граммы, имеющие такое
же начало, то есть такие же первые N-1 символов.

Создайте экземпляр класса ``NGramLanguageModel`` со следующими аргументами:

* Закодированный текст, который принимает значение ``None``;
* Размер N-грамм.

Установите значение защищенного атрибута ``self._n_gram_frequencies`` класса
``NGramLanguageModel``

.. note:: Если на вход подается некорректное значение размера N-грамм (значение
          не относится к типу ``int``, аргумент не содержит никакого значения или
          размер находится за пределами от 2 до 5), то метод возвращает ``None``.

Метод обязательно должен вызывать методы
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.get_id` и
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel.set_n_grams`.

Шаг 8.1.2 Получить экземпляр класса ``TextProcessor``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

В данном классе атрибут ``self._text_processor`` является защищенным:
мы не хотим допустить его изменения в процессе использования внешним пользователем.
Однако узнать, какой именно токен используется для разделения слов, все-таки
иногда необходимо.

Для этого реализуем метод
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModelReader.get_text_processor`,
который возвращает значение внутреннего атрибута ``self._text_processor``.
Метод не принимает никаких аргументов.

Шаг 8.2. Объявить сущность по созданию BackOff генератора
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Создайте класс :py:class:`lab_3_generate_by_ngrams.main.BackOffGenerator`.

Описание внутренних атрибутов класса:

* ``self._language_models`` - словарь, ключом которого является размер
  N–грамм, а значением экземпляр класса ``NGramLanguageModel``. Экземпляр
  класса вы можете получить из списка ``language_models``.
* ``self._text_processor`` - экземпляр класса ``TextProcessor``.

Шаг 8.2.1 Получить буквы-кандидаты для генерации
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуем метод
:py:meth:`lab_3_generate_by_ngrams.main.BackOffGenerator._get_next_token`,
который возвращает словарь, ключами которого являются буквы-кандидаты, а значениями
частоты N-грамм, из которых данная буква была выявлена по контексту.

Вам необходимо:

1. Получить все возможные размеры N–грамм, используя внутренний
   атрибут данного класса и отсортировать его в порядке убывания N–грамм;
2. Для каждой N–граммы получить экземпляр класса ``NGramLanguageModel``;
3. Получить контекст c помощью языковой модели для дальнейшей генерации буквы-кандидата;
4. Получить все буквы-кандидаты для генерации в виде словаря;
5. Перейти к N–грамме меньшего размера и повторить шаги 2-4,

.. note:: Если на вход подается некорректное значение аргумента (аргумента
          не является кортежем или кортеж пустой), то метод возвращает ``None``.
          Если список размеров N–грамм пустой или вызываемые методы возвращают ``None``,
          то метод также возвращает ``None``

В данном методе необходимо использовать методы
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel.get_n_gram_size` и
:py:meth:`lab_3_generate_by_ngrams.main.NGramLanguageModel.generate_next_token`.

**Обратите внимание**, что данный метод является защищенным: его использование за
пределами методов класса не предполагается.

Шаг 8.2.2 Генерация последовательности
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Реализуйте метод
:py:meth:`lab_3_generate_by_ngrams.main.BackOffGenerator.run`,
который принимает на вход длину последовательности для генерации
и начало последовательности в строковом виде.

.. note:: Если на вход подаются некорректные значения (количество букв для
          генерации не является целым числом, а также последовательность не является
          строкой или строка пустая), то метод возвращает ``None``.

Перед тем, как непосредственно приступить к генерации, необходимо закодировать заданную
последовательность (``prompt``).

Далее Вам следует:

1. Получить все буквы-кандидаты для генерации. Используйте уже реализованный метод
   класса ``BackOffGenerator``;
2. Сгенерировать указанное количество букв, каждый раз выбирая
   кандидата с наибольшей вероятностью из словаря и добавляя его к последовательности.

Следует учесть, что в случае, если буквы-кандидаты не были найдены, то генерация
прекращается.

Метод возвращает сгенерированный текст в виде строки.

.. note:: Если на вход подаются некорректные значения (длина последовательности
          не типа ``int``, заданная последовательность не является
          строкой или последовательность пустая), если вызываемые методы возвращают
          значение ``None``, возвращается ``None``.

В данном методе необходимо использовать методы
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.encode` и
:py:meth:`lab_3_generate_by_ngrams.main.TextProcessor.decode`,
а также метод
:py:meth:`lab_3_generate_by_ngrams.main.BackOffGenerator._get_next_token`.


Полезные ссылки
---------------

- `Видео с объяснением алгоритма Beam Search
  <https://www.youtube.com/watch?v=3m1lvN2fTtY>`__
- `Оригинальная статья о Beam Search
  алгоритме <https://www.researchgate.net/publication/257618443_Beam_search_algorithms_for_multilabel_learningv>`__
- `Описание формата хранения данных JSON <https://ru.wikipedia.org/wiki/JSON>`__
   и `документация библиотеки <https://pythonworld.ru/moduli/modul-json.html>`__
   для работы с такими файлами
-  `Видео и статья с объяснением жадного
   алгоритма <https://www.simplilearn.com/tutorials/
   data-structure-tutorial/greedy-algorithm#example_of_greedy_algorithm>`__
